{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from sklearn import preprocessing\n",
    "from scipy.io.wavfile import read\n",
    "import python_speech_features as mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have used the stack stack overflow answer which was given in the question as the reference\n",
    "# (Ref- https://stackoverflow.com/questions/54160128/feature-extraction-using-mfcc)\n",
    "def delta_calculation(feature_matrix):\n",
    "    num_frames,num_features=feature_matrix.shape\n",
    "    delta_features=np.zeros((num_frames, num_features))  \n",
    "    window_size=2 \n",
    "    for frame_idx in range(num_frames):\n",
    "        delta_sum=np.zeros(num_features)\n",
    "        for n in range(1,window_size+1):\n",
    "            prev_idx=max(0,frame_idx-n) \n",
    "            next_idx=min(num_frames-1,frame_idx+n)\n",
    "            delta_sum+=(n *(feature_matrix[next_idx]-feature_matrix[prev_idx]))\n",
    "        delta_features[frame_idx]=delta_sum/(2*window_size*(window_size+1))  # Normalize by the total weight\n",
    "    return delta_features\n",
    "# Function to extract MFCC features\n",
    "def extract_features(audio,rate):\n",
    "    #Input parametres for mfcc.mfcc\n",
    "    frame_size=0.025 \n",
    "    frame_stride=0.01 \n",
    "    num_mfcc_features=20\n",
    "    nfft=1200 \n",
    "    append_energy=True  \n",
    "    mfcc_features=mfcc.mfcc(audio,rate,winlen=frame_size,winstep=frame_stride,numcep=num_mfcc_features,nfft=nfft,appendEnergy=append_energy)\n",
    "    mfcc_features=preprocessing.scale(mfcc_features)\n",
    "    delta_features=delta_calculation(mfcc_features)\n",
    "    combined_features=np.hstack((mfcc_features,delta_features))\n",
    "    return combined_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before writing this code I have checked online to study and understand  about implementing GMM on features of audio samples\n",
    "# So the acknowledgements are https://github.com/shivam-shukla/Speaker-Recognition-Using-GMM-MFCC-Python3\n",
    "\n",
    "# Function to train GMM models for each speaker\n",
    "def train_gmm(source_dir,model_save_dir,training_file):\n",
    "    with open(training_file,'r') as file_paths:\n",
    "        file_count=1 \n",
    "        accumulated_features=np.asarray(())\n",
    "        for file_path in file_paths:\n",
    "            file_path=file_path.strip()\n",
    "            sample_rate,audio_data=read(os.path.join(source_dir,file_path))\n",
    "            feature_vectors=extract_features(audio_data,sample_rate)  # Extract features from the audio\n",
    "            if accumulated_features.size==0:\n",
    "                accumulated_features=feature_vectors\n",
    "            else:\n",
    "                accumulated_features=np.vstack((accumulated_features,feature_vectors))\n",
    "            # Training GMM after processing 5 files for each speaker\n",
    "            if file_count==5:\n",
    "                gmm_model=GMM(n_components=5,covariance_type='diag',n_init=3)\n",
    "                gmm_model.fit(accumulated_features)\n",
    "                speaker_name=file_path.split(\"-\")[0] \n",
    "                model_filename=f\"{speaker_name}.gmm\"\n",
    "                np.save(os.path.join(model_save_dir,model_filename),gmm_model)\n",
    "                print(f'The GMM model for speaker \"{speaker_name}\" has {accumulated_features.shape} as shape.')\n",
    "                # Resetting for the next speaker's files\n",
    "                accumulated_features=np.asarray(())\n",
    "                file_count = 0\n",
    "            file_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_speakers(test_audio_dir,model_dir,test_file_list):\n",
    "    with open(test_file_list, 'r') as test_files:\n",
    "        # Load GMM models saved in .npy format\n",
    "        gmm_model_files=[os.path.join(model_dir,file) for file in os.listdir(model_dir) if file.endswith('.npy')]\n",
    "        gmm_models=[np.load(model_file,allow_pickle=True).item() for model_file in gmm_model_files]\n",
    "        speaker_names=[os.path.basename(model_file).split(\".gmm\")[0] for model_file in gmm_model_files]\n",
    "        actual_speaker_labels=[] \n",
    "        predicted_speaker_labels=[]\n",
    "        # Process each test file for speaker identification\n",
    "        for audio_file in test_files:\n",
    "            audio_file=audio_file.strip()\n",
    "            sample_rate,audio_data=read(os.path.join(test_audio_dir,audio_file))\n",
    "            feature_vector=extract_features(audio_data,sample_rate)\n",
    "            log_likelihoods=np.zeros(len(gmm_models)) \n",
    "            # Evaluating the audio features against each GMM model\n",
    "            for i, gmm in enumerate(gmm_models):\n",
    "                log_likelihoods[i]=np.sum(gmm.score(feature_vector))  # Sum of log likelihood for the current model\n",
    "            best_match_idx=np.argmax(log_likelihoods)\n",
    "            predicted_speaker=speaker_names[best_match_idx]\n",
    "            predicted_speaker_labels.append(predicted_speaker)\n",
    "            actual_speaker=audio_file.split(\"-\")[0][:len(predicted_speaker)]\n",
    "            actual_speaker_labels.append(actual_speaker)\n",
    "            print(f\"Processed audio file: {audio_file}\")\n",
    "            print(f\"Identified speaker: {predicted_speaker} (Best match)\")\n",
    "            print(f\"Actual speaker in the recording: {actual_speaker}\\n\")\n",
    "        correct_predictions=sum([1 for actual, predicted in zip(actual_speaker_labels,predicted_speaker_labels) if actual==predicted])\n",
    "        accuracy = (correct_predictions/len(actual_speaker_labels)) * 100\n",
    "        print(f\"Speaker identification completed for {len(actual_speaker_labels)} audio files.\")\n",
    "        print(f\"Overall identification accuracy: {accuracy:.2f}%\\n\")\n",
    "    return actual_speaker_labels,predicted_speaker_labels,accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The GMM model for speaker \"Abhay\" has (5439, 40) as shape.\n",
      "The GMM model for speaker \"Eknath\" has (6079, 40) as shape.\n",
      "The GMM model for speaker \"Rg\" has (5099, 40) as shape.\n",
      "The GMM model for speaker \"Vaibhav\" has (6334, 40) as shape.\n",
      "The GMM model for speaker \"Rishika\" has (6079, 40) as shape.\n",
      "The GMM model for speaker \"ShivamY\" has (8967, 40) as shape.\n",
      "\n",
      "\n",
      "Processed audio file: Abhay_audio1.wav\n",
      "Identified speaker: Abhay (Best match)\n",
      "Actual speaker in the recording: Abhay\n",
      "\n",
      "Processed audio file: Abhay_audio2.wav\n",
      "Identified speaker: Abhay (Best match)\n",
      "Actual speaker in the recording: Abhay\n",
      "\n",
      "Processed audio file: Abhay_audio3.wav\n",
      "Identified speaker: Abhay (Best match)\n",
      "Actual speaker in the recording: Abhay\n",
      "\n",
      "Processed audio file: Abhay_audio4.wav\n",
      "Identified speaker: Abhay (Best match)\n",
      "Actual speaker in the recording: Abhay\n",
      "\n",
      "Processed audio file: Abhay_audio5.wav\n",
      "Identified speaker: Abhay (Best match)\n",
      "Actual speaker in the recording: Abhay\n",
      "\n",
      "Processed audio file: Eknath_audio1.wav\n",
      "Identified speaker: Eknath (Best match)\n",
      "Actual speaker in the recording: Eknath\n",
      "\n",
      "Processed audio file: Eknath_audio2.wav\n",
      "Identified speaker: Eknath (Best match)\n",
      "Actual speaker in the recording: Eknath\n",
      "\n",
      "Processed audio file: Eknath_audio3.wav\n",
      "Identified speaker: Eknath (Best match)\n",
      "Actual speaker in the recording: Eknath\n",
      "\n",
      "Processed audio file: Eknath_audio4.wav\n",
      "Identified speaker: Eknath (Best match)\n",
      "Actual speaker in the recording: Eknath\n",
      "\n",
      "Processed audio file: Eknath_audio5.wav\n",
      "Identified speaker: Eknath (Best match)\n",
      "Actual speaker in the recording: Eknath\n",
      "\n",
      "Processed audio file: Rg_audio1.wav\n",
      "Identified speaker: Rg (Best match)\n",
      "Actual speaker in the recording: Rg\n",
      "\n",
      "Processed audio file: Rg_audio2.wav\n",
      "Identified speaker: Rg (Best match)\n",
      "Actual speaker in the recording: Rg\n",
      "\n",
      "Processed audio file: Rg_audio3.wav\n",
      "Identified speaker: Rg (Best match)\n",
      "Actual speaker in the recording: Rg\n",
      "\n",
      "Processed audio file: Rg_audio4.wav\n",
      "Identified speaker: Rg (Best match)\n",
      "Actual speaker in the recording: Rg\n",
      "\n",
      "Processed audio file: Rg_audio5.wav\n",
      "Identified speaker: Rg (Best match)\n",
      "Actual speaker in the recording: Rg\n",
      "\n",
      "Processed audio file: Rishika_audio1.wav\n",
      "Identified speaker: Rishika (Best match)\n",
      "Actual speaker in the recording: Rishika\n",
      "\n",
      "Processed audio file: Rishika_audio2.wav\n",
      "Identified speaker: Rishika (Best match)\n",
      "Actual speaker in the recording: Rishika\n",
      "\n",
      "Processed audio file: Rishika_audio2.wav\n",
      "Identified speaker: Rishika (Best match)\n",
      "Actual speaker in the recording: Rishika\n",
      "\n",
      "Processed audio file: Rishika_audio4.wav\n",
      "Identified speaker: Rishika (Best match)\n",
      "Actual speaker in the recording: Rishika\n",
      "\n",
      "Processed audio file: Rishika_audio5.wav\n",
      "Identified speaker: Rishika (Best match)\n",
      "Actual speaker in the recording: Rishika\n",
      "\n",
      "Processed audio file: ShivamY_audio1.wav\n",
      "Identified speaker: ShivamY (Best match)\n",
      "Actual speaker in the recording: ShivamY\n",
      "\n",
      "Processed audio file: ShivamY_audio2.wav\n",
      "Identified speaker: ShivamY (Best match)\n",
      "Actual speaker in the recording: ShivamY\n",
      "\n",
      "Processed audio file: ShivamY_audio3.wav\n",
      "Identified speaker: ShivamY (Best match)\n",
      "Actual speaker in the recording: ShivamY\n",
      "\n",
      "Processed audio file: ShivamY_audio4.wav\n",
      "Identified speaker: ShivamY (Best match)\n",
      "Actual speaker in the recording: ShivamY\n",
      "\n",
      "Processed audio file: ShivamY_audio5.wav\n",
      "Identified speaker: ShivamY (Best match)\n",
      "Actual speaker in the recording: ShivamY\n",
      "\n",
      "Processed audio file: Vaibhav_audio1.wav\n",
      "Identified speaker: Vaibhav (Best match)\n",
      "Actual speaker in the recording: Vaibhav\n",
      "\n",
      "Processed audio file: Vaibhav_audio2.wav\n",
      "Identified speaker: Vaibhav (Best match)\n",
      "Actual speaker in the recording: Vaibhav\n",
      "\n",
      "Processed audio file: Vaibhav_audio3.wav\n",
      "Identified speaker: Vaibhav (Best match)\n",
      "Actual speaker in the recording: Vaibhav\n",
      "\n",
      "Processed audio file: Vaibhav_audio4.wav\n",
      "Identified speaker: Vaibhav (Best match)\n",
      "Actual speaker in the recording: Vaibhav\n",
      "\n",
      "Processed audio file: Vaibhav_audio5.wav\n",
      "Identified speaker: Vaibhav (Best match)\n",
      "Actual speaker in the recording: Vaibhav\n",
      "\n",
      "Speaker identification completed for 30 audio files.\n",
      "Overall identification accuracy: 100.00%\n",
      "\n",
      "\n",
      "GMM Model Accuracy for test audio files: 100.0%\n",
      "List of Actual Speakers: ['Abhay', 'Abhay', 'Abhay', 'Abhay', 'Abhay', 'Eknath', 'Eknath', 'Eknath', 'Eknath', 'Eknath', 'Rg', 'Rg', 'Rg', 'Rg', 'Rg', 'Rishika', 'Rishika', 'Rishika', 'Rishika', 'Rishika', 'ShivamY', 'ShivamY', 'ShivamY', 'ShivamY', 'ShivamY', 'Vaibhav', 'Vaibhav', 'Vaibhav', 'Vaibhav', 'Vaibhav']\n",
      "List of Detected Speakers: ['Abhay', 'Abhay', 'Abhay', 'Abhay', 'Abhay', 'Eknath', 'Eknath', 'Eknath', 'Eknath', 'Eknath', 'Rg', 'Rg', 'Rg', 'Rg', 'Rg', 'Rishika', 'Rishika', 'Rishika', 'Rishika', 'Rishika', 'ShivamY', 'ShivamY', 'ShivamY', 'ShivamY', 'ShivamY', 'Vaibhav', 'Vaibhav', 'Vaibhav', 'Vaibhav', 'Vaibhav']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "source=\"GMM-Speaker-Identification-data/Speaker_data/Voice_Samples_Training\"  # Path of folder containing training audio\n",
    "dest=\"Trained_Speech_Models\"  # Path of folder to save trained models\n",
    "train_file=\"Voice_Samples_Training_Path.txt\"  # File listing training data\n",
    "test_file=\"Testing_audio_Path.txt\"  # File listing testing data\n",
    "# Train GMM models\n",
    "train_gmm(source,dest,train_file)\n",
    "print(\"\\n\")\n",
    "#Test speaker identification and get accuracy\n",
    "actual_speakers,detected_speakers,accuracy=evaluate_speakers(\"GMM-Speaker-Identification-data/Speaker_data/Testing_Audio\", dest, test_file)\n",
    "print(f\"\\nGMM Model Accuracy for test audio files: {accuracy}%\")\n",
    "print(f\"List of Actual Speakers: {actual_speakers}\")\n",
    "print(f\"List of Detected Speakers: {detected_speakers}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
